{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造词向量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们借鉴了word embedding的思路去改进成自己的char embedding，即将一个字母看成是一个向量，而一个单词就是由多个字母组成的，也就是由多个向量组成的一个矩阵，我们利用这个矩阵来表示一个单词之后就可以训练出自己的从而看字母之间的关联程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从https://raw.githubusercontent.com/dwyl/english-words/master/words.txt这个网址下载英文单词表\n",
    "# 然后训练一个我们的字母嵌入模型\n",
    "# 用于将字母转换为向量\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载单词表到本地\n",
    "# !wget https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
    "# 已经下载好了，不需要再下载了，文件名为words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open('words.txt', 'r')\n",
    "words = open_file.read().split('\\n')\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of words:  <class 'list'>\n",
      "Number of words:  466551\n",
      "First 10 words:  ['2', '1080', '&c', '10-point', '10th', '11-point', '12-point', '16-point', '18-point', '1st']\n"
     ]
    }
   ],
   "source": [
    "print('type of words: ', type(words))\n",
    "print('Number of words: ', len(words))\n",
    "print('First 10 words: ', words[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 将所有的单词转换为小写\n",
    "words = [word.lower() for word in words]\n",
    "# 去掉单词中的空格、特殊字符、数字\n",
    "words = [word for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of words:  <class 'list'>\n",
      "Number of words:  416296\n",
      "First 10 words:  ['a', 'aa', 'aaa', 'aaaa', 'aaaaaa', 'aaal', 'aaas', 'aaberg', 'aachen', 'aae']\n"
     ]
    }
   ],
   "source": [
    "print('type of words: ', type(words))\n",
    "print('Number of words: ', len(words))\n",
    "print('First 10 words: ', words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存预处理后的单词表\n",
    "# with open('words_clean.txt', 'w') as f:\n",
    "#     for word in words:\n",
    "#         f.write(word + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字母向量（词向量）\n",
    "\n",
    "在自然语言处理中(NLP)，我们拥有很多个英文单词的时候是不可能只用一个one-hot向量去表示的了，而是采取了embedding的方法，将数据投影在规定的维度上，也就是我们定义这种方法为词嵌入模型，利用类似于词嵌入的方法，我们提出了字母嵌入，即将26个字母也映射到统一的维度上，然后计算每一个字母都是一个向量，那么向量的夹角我们就是字母的相关程度，通过统计一个单词中俩俩字母的相关程度，我们就可以得出该单词的怪异程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define characters and indices\n",
    "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "char2ind = {char: index for index, char in enumerate(chars)}\n",
    "ind2char = {index: char for index, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预处理后的单词表\n",
    "open_file = open('words_clean.txt', 'r')\n",
    "words_clean = open_file.read().split('\\n')\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_word_len:  45\n"
     ]
    }
   ],
   "source": [
    "# Create character tensor for each word\n",
    "max_word_len = max([len(word) for word in words_clean])\n",
    "print('max_word_len: ', max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25904/3727256720.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar2ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_char = torch.zeros((len(words_clean), max_word_len), dtype=torch.long)\n",
    "for i, word in enumerate(words_clean):\n",
    "  for j, char in enumerate(word):\n",
    "    X_char[i, j] = char2ind[char]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选取几个单词，查看其对应的字符张量\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(words_clean))\n",
    "    print(\"word: \", words_clean[idx])\n",
    "    print(\"char tensor: \", X_char[idx])\n",
    "\n",
    "# 随机选取几个单词制作一个小样本\n",
    "# 用于后面的训练\n",
    "sample_size = 20\n",
    "idx = np.random.randint(0, len(words_clean), sample_size)\n",
    "words_sample = [words_clean[i] for i in idx]\n",
    "X_char_sample = X_char[idx]\n",
    "for i in range(sample_size):\n",
    "    print(\"word: \", words_sample[i])\n",
    "    print(\"char tensor: \", X_char_sample[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "char_embedding_size = 3 # arbitrary choice\n",
    "hidden_size = 50 # arbitrary choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(chars), embedding_dim=char_embedding_size)\n",
    "bidirectional_layer = nn.LSTM(input_size=char_embedding_size, hidden_size=hidden_size, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for each word\n",
    "embeddings = embedding_layer(X_char)\n",
    "output, (hidden, cell) = bidirectional_layer(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define two characters and their indices\n",
    "char1 = \"h\"\n",
    "char2 = \"w\"\n",
    "ind1 = char2ind[char1]\n",
    "ind2 = char2ind[char2]\n",
    "\n",
    "# Get embeddings for each character\n",
    "emb1 = embedding_layer(torch.tensor(ind1))\n",
    "emb2 = embedding_layer(torch.tensor(ind2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "similarity = F.cosine_similarity(emb1, emb2, dim=-1)\n",
    "print(similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# 导入PyTorch库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义单词列表和索引映射\n",
    "word_list = [\"apple\", \"banana\", \"strawberry\"]\n",
    "word_to_idx = {\"apple\": 0, \"banana\": 1, \"strawberry\": 2}\n",
    "\n",
    "# 定义嵌入矩阵的大小和随机种子\n",
    "embedding_size = 3\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 创建一个嵌入层对象，初始化嵌入矩阵为随机值\n",
    "embedding = nn.Embedding(len(word_list), embedding_size)\n",
    "\n",
    "# 打印初始的嵌入矩阵\n",
    "print(\"Initial embedding matrix:\")\n",
    "print(embedding.weight)\n",
    "\n",
    "# 定义一个简单的神经网络模型，包含一个嵌入层和一个线性层\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = embedding # 使用已有的嵌入层对象\n",
    "        self.linear = nn.Linear(embedding_size, 1) # 定义一个线性层，输出一个标量\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # 将输入的索引转换为嵌入向量\n",
    "        x = self.linear(x) # 将嵌入向量输入到线性层，得到输出标量\n",
    "        return x\n",
    "\n",
    "# 创建一个模型对象，并将其移动到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(embedding).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss() # 使用均方误差作为损失函数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # 使用随机梯度下降作为优化器\n",
    "\n",
    "# 定义训练数据和标签（这里只是随便定义了一些数据，你可以根据你的任务来定义）\n",
    "inputs = torch.tensor([0, 1, 2]).to(device) # 输入三个单词的索引\n",
    "labels = torch.tensor([0.5, -0.5, 0.8]).to(device) # 输出三个标量作为标签\n",
    "\n",
    "# 训练模型（这里只训练了10个epoch，你可以根据你的需要调整）\n",
    "epochs = 10 \n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() # 清空梯度缓存\n",
    "    outputs = model(inputs) # 前向传播，得到模型输出\n",
    "    loss = criterion(outputs.squeeze(), labels) # 计算损失值\n",
    "    loss.backward() # 反向传播，计算梯度值\n",
    "    optimizer.step() # 更新参数值\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, loss: {loss.item():.4f}\") # 打印每个epoch的损失值\n",
    "\n",
    "# 打印训练后的嵌入矩阵（注意这里只是演示，实际上可能并没有收敛）\n",
    "print(\"Final embedding matrix:\")\n",
    "print(embedding.weight)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
